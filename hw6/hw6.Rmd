---
title: "Homework 6"
author: "Painter, Ty"
date: "`r date()`"
output: github_document
editor_options: 
  chunk_output_type: console
---

Goal: Understand and implement a random forest classifier.

Using the “vowel.train” data, and the “randomForest” function in the R package “randomForest”. Develop a random forest classifier for the vowel data by doing the following:
```{r}
library(randomForest)
library(tidyverse)
library(caret)
library(heuristica)
train <- read.csv(url("https://web.stanford.edu/~hastie/ElemStatLearn/datasets/vowel.train"))
train <- train[,2:ncol(train)]
```

### 1. Convert the response variable in the “vowel.train” data frame to a factor variable prior to training, so that “randomForest” does classification rather than regression.
```{r}
train$y <- as.factor(train$y)
```

### 2. Review the documentation for the “randomForest” function.

### 3. Fit the random forest model to the vowel data using all of the 11 features using the default values of the tuning parameters.
```{r}
fit <- randomForest(y ~ ., data=train)
```

### 4. Use 5-fold CV and tune the model by performing a grid search for the following tuning parameters: 
1) the number of variables randomly sampled as candidates at each split; consider values 3, 4, and 5. 
2) the minimum size of terminal nodes; consider a sequence (1, 5, 10, 20, 40, and 80).
```{r}
samp_vars <- c(3,4,5)
node_size <- c(1, 5, 10, 20, 40, 80)

# 5 folds 
control <- trainControl(method='cv', 
                        number=5,
                        search="grid")
set.seed(41)
# grid search on mtry and nodesize
tunegrid <- expand.grid(.mtry = samp_vars, 
                        .min.node.size = node_size, 
                        .splitrule = "gini")
rf_fit <- train(y~., 
                data = train, 
                trControl = control,
                metric = "Accuracy",
                method = 'ranger', 
                tuneGrid = tunegrid, 
                classification = TRUE)

tune_params <- as.data.frame(rf_fit[4]) # make grid search accuracy a data frame
tune_params <- tune_params %>% arrange(desc(results.Accuracy)) # sort by accuracy and select the params with best accuracy
tune_params[1,]
```
I created a data frame with the results from the grid search and arranged the results in order of highest accuracy and selected the top row. The parameters with the best accuracy are 3 variables sampled at each split and a minimum of 1 terminal node.
```{r}
set.seed(12)
rf_best_fit <- randomForest(y ~ ., 
                            data=train,
                            nodesize=1,
                            mtry=3)
```
I created a best fit model with the specified best parameters.

### 5. With the tuned model, make predictions using the majority vote method, and compute the misclassification rate using the ‘vowel.test’ data.
```{r}
test <- read.csv(url("https://web.stanford.edu/~hastie/ElemStatLearn/datasets/vowel.test"))
test <- test[,2:ncol(test)]
test_y <- as.factor(test$y)
## compute test error on testing data
test_preds <- predict(rf_best_fit, newdata = test)
cverr <- rep(NA, length(test))

confusionMatrix(test_y, test_preds) 
```
The misclassification rate is ~40%.
```{r}
# misclassification rate
accuracy <- as.data.frame(confusionMatrix(test_y, test_preds)[3])[1,]
1 - accuracy
```




