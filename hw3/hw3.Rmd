---
title: "Homework 3"
author: "Painter, Ty"
date: "`r date()`"
output: github_document
---
# Using the RMarkdown/knitr/github mechanism, implement the following tasks:
- Use the prostate cancer data.
```{r}
prostate <-read.table(url('https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data'))
prostate_train <- subset(prostate, train==TRUE) # 2/3 for train, 1/3 test
```

- Use the cor function to reproduce the correlations listed in HTF Table 3.1, page 50.
```{r}
prostate.x <- prostate_train[,c("lweight", "age", "lbph", "svi", "lcp", "gleason", "pgg45")]
prostate.y <- prostate_train[,c("lcavol", "lweight", "age", "lbph", "svi", "lcp", "gleason")]
cor_table <- round(cor(prostate.x, prostate.y),3)

cor_table[upper.tri(cor_table)]<-""
cor_table<-as.data.frame(cor_table)
cor_table
```

- Treat lcavol as the outcome, and use all other variables in the data set as predictors.
- With the training subset of the prostate data, train a least-squares regression model with all predictors using the lm function.
```{r}
## predict lcavol consider all other predictors
## lm fits using L2 loss
ls_model <- lm(lcavol ~ ., data=prostate_train) # . means use to predict everything
summary(ls_model)
coef(ls_model)
residuals(ls_model)
```

- Use the testing subset to compute the test error (average squared-error loss) using the fitted least-squares regression model.
```{r}
prostate_test <- subset(prostate, train==FALSE) # 2/3 for train, 1/3 test

## functions to compute testing/training error w/lm for squared error loss
L2_loss <- function(y, yhat) {
  (y-yhat)^2 # lasso and ridge use squared error loss; when compute test error use same error formula used for train error  
}
  
ls_error <- function(data, fit, loss=L2_loss){
  mean(loss(data$lcavol, predict(fit, newdata=data))) # average of squared error loss function, predict applies ls_model to test data
}
  
## testing error 
ls_error(prostate_test, ls_model) # test error is bigger than train
```

- Train a ridge regression model using the glmnet function, and tune the value of lambda (i.e., use guess and check to find the value of lambda that approximately minimizes the test error).
```{r}
library('glmnet') 
## use glmnet to fit lasso
## glmnet fits using penalized L2 loss
## first create an input matrix and output vector
form <- lcavol ~ 0 + lweight + age + lbph + lcp + pgg45 + lpsa + svi + gleason # what is this?
x_inp <- model.matrix(form, data=prostate_train) # math to form this matrix?
y_out <- prostate_train$lcavol
ridge_reg <- glmnet(x=x_inp, y=y_out, lambda=seq(0.5, 0, -0.05))
print(ridge_reg$beta) # beta over sequence of lambda

## functions to compute testing/training error with glmnet
ridge_error <- function(data, fit, lambda, form, loss=L2_loss) {
  x_inp <- model.matrix(form, data=data)
  y_out <- data$lcavol
  y_hat <- predict(ridge_reg, newx=x_inp, s=lambda)  ## see predict.elnet, s = values of penalty parameter
  mean(loss(y_out, y_hat)) # avg L2 loss function
}

## minimize test error
ridge_error(prostate_test, ridge_reg, lambda=.21, form=form) 
```

- Create a figure that shows the training and test error associated with ridge regression as a function of lambda
```{r}
## compute training and testing errors as function of lambda
train_err <- sapply(ridge_reg$lambda, function(lambda) 
  ridge_error(prostate_train, fit, lambda, form))
test_err <- sapply(ridge_reg$lambda, function(lambda) 
  ridge_error(prostate_test, fit, lambda, form))

## plot test/train error
plot(x=range(ridge_reg$lambda),
     y=range(c(train_err, test_err)),
     type='n',
     xlab=expression(lambda),
     ylab='train/test error')
points(ridge_reg$lambda, train_err, pch=19, type='b', col='darkblue')
points(ridge_reg$lambda, test_err, pch=19, type='b', col='darkred')
legend('topleft', c('train','test'), lty=1, pch=19,
       col=c('darkblue','darkred'), bty='n')

colnames(ridge_reg$beta) <- paste('lam =', ridge_reg$lambda) # test error minimized at 0.21
```

- Create a path diagram of the ridge regression analysis, similar to HTF Figure 3.8
```{r}
## plot path diagram
plot(x=range(ridge_reg$lambda),
     y=range(as.matrix(ridge_reg$beta)),
     type='n',
     xlab=expression(lambda),
     ylab='Coefficients')
for(i in 1:nrow(ridge_reg$beta)) {
  points(x=ridge_reg$lambda, y=ridge_reg$beta[i,], pch=19, col='#00000055')
  lines(x=ridge_reg$lambda, y=ridge_reg$beta[i,], col='#00000055')
}
abline(h=0, lty=3, lwd=2)
```

## Links
(https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/about)